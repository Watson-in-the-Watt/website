---
layout: post
title: Dimension Reduction
category: blog
permalink: /blogs/DimensionReduction
author:
  name: Quinn Hubbarth
  email: "qhubbar@clemson.edu"
---

Dimension reduction, also known as dimensionality reduction, is the process of reducing the amount of variables in a dataset to make the data easier to use. The resulting values from dimension reduction will be representations of the most important features in a dataset. Reduced dimensions are often more efficient in machine learning algorithms, although some data is lost when reducing dimensions. There are many different techniques and uses for dimension reduction; notably, dimension reduction is frequently used for data visualization.

Generally, the simplified datasets produced by dimension reduction are extremely effective for data visualization. For example, if a data point with 100 variables is reduced to just 2, it can be plotted on an 2D graph, with the dimensions set to X and Y. Although many variables are slimmed down using this technique of visualization, it is often easy to see patterns in the data.

This article will outline how dimension reduction can be effective for data visualization for a variety of contexts, and explain how different methods for dimension reduction can be vastly different.

# Methods

There are many different methods machine learning researchers can use to perform dimension reduction. Each method has its pros and cons. Some methods are extremely accurate, while others are extremely fast; others are great for niche tasks. We chose to test four different dimension reduction methods:

### PCA
 * Principal component analysis
 * Used for over 100 years in mathematics and statistics
 * Simple, efficient algorithm that can run very quickly
 * Available in sci-kit learn libraries 
### SVD
 * Singular value decomposition
 * Similar to PCA in usage/efficiency
 * Available in sci-kit learn libraries 
### T-SNE
 * t-distributed stochastic neighbor embedding
 * Newer method; invented in 2008
 * Uses more complicated mathematics such as manifold learning
 * More computationally expensive than PCA or SVD
### UMAP
 * Uniform manifold approximation and projection
 * Technique invented in 2018 which expands on T-SNE
 * Similar to T-SNE in effectiveness
 * Attempts to be more efficient than T-SNE; still much longer runtime than PCA or SVD


# Datasets/Results

To show how vastly applicable these techniques are, we chose three vastly different datasets. These datasets are frequently used as benchmarks for different machine learning algorithms. Additionally, they each are not purely numerical datasets. Each dataset had to be translated to purely numerical datasets through some form of data manipulation so that dimension reduction could be performed.

### AMES Housing
 * House price data (tabular data)
 * 81 columns of categorical and continuous data to describe a house
 * All continuous data and select categorical data used in reduction
 * Categorical data must be translated into numerical data through “dummy encoding”
 
Example data:
|    |   Lot Frontage |   Lot Area |   Overall Qual |   Overall Cond |   Year Built |   Year Remod/Add |   Mas Vnr Area |   BsmtFin SF 1 |   BsmtFin SF 2 |   Bsmt Unf SF |   Total Bsmt SF |   1st Flr SF |   2nd Flr SF |   Low Qual Fin SF |   Gr Liv Area |   Bsmt Full Bath |   Bsmt Half Bath |   Full Bath |   Half Bath |   Bedroom AbvGr |   Kitchen AbvGr |   TotRms AbvGrd |   Fireplaces |   Garage Yr Blt |   Garage Cars |   Garage Area |   Wood Deck SF |   Open Porch SF |   Enclosed Porch |   3Ssn Porch |   Screen Porch |   Pool Area |   Misc Val |   Mo Sold |   Yr Sold |   House Style_1.5Fin |   House Style_1.5Unf |   House Style_1Story |   House Style_2.5Fin |   House Style_2.5Unf |   House Style_2Story |   House Style_SFoyer |   House Style_SLvl |   Kitchen Qual_Ex |   Kitchen Qual_Fa |   Kitchen Qual_Gd |   Kitchen Qual_Po |   Kitchen Qual_TA |   Utilities_AllPub |   Utilities_NoSeWa |   Utilities_NoSewr |
|---:|---------------:|-----------:|---------------:|---------------:|-------------:|-----------------:|---------------:|---------------:|---------------:|--------------:|----------------:|-------------:|-------------:|------------------:|--------------:|-----------------:|-----------------:|------------:|------------:|----------------:|----------------:|----------------:|-------------:|----------------:|--------------:|--------------:|---------------:|----------------:|-----------------:|-------------:|---------------:|------------:|-----------:|----------:|----------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|-------------------:|------------------:|------------------:|------------------:|------------------:|------------------:|-------------------:|-------------------:|-------------------:|
|  0 |            141 |      31770 |              6 |              5 |         1960 |             1960 |            112 |            639 |              0 |           441 |            1080 |         1656 |            0 |                 0 |          1656 |                1 |                0 |           1 |           0 |               3 |               1 |               7 |            2 |            1960 |             2 |           528 |            210 |              62 |                0 |            0 |              0 |           0 |          0 |         5 |      2010 |                    0 |                    0 |                    1 |                    0 |                    0 |                    0 |                    0 |                  0 |                 0 |                 0 |                 0 |                 0 |                 1 |                  1 |                  0 |                  0 |
|  1 |             80 |      11622 |              5 |              6 |         1961 |             1961 |              0 |            468 |            144 |           270 |             882 |          896 |            0 |                 0 |           896 |                0 |                0 |           1 |           0 |               2 |               1 |               5 |            0 |            1961 |             1 |           730 |            140 |               0 |                0 |            0 |            120 |           0 |          0 |         6 |      2010 |                    0 |                    0 |                    1 |                    0 |                    0 |                    0 |                    0 |                  0 |                 0 |                 0 |                 0 |                 0 |                 1 |                  1 |                  0 |                  0 |
|  2 |             81 |      14267 |              6 |              6 |         1958 |             1958 |            108 |            923 |              0 |           406 |            1329 |         1329 |            0 |                 0 |          1329 |                0 |                0 |           1 |           1 |               3 |               1 |               6 |            0 |            1958 |             1 |           312 |            393 |              36 |                0 |            0 |              0 |           0 |      12500 |         6 |      2010 |                    0 |                    0 |                    1 |                    0 |                    0 |                    0 |                    0 |                  0 |                 0 |                 0 |                 1 |                 0 |                 0 |                  1 |                  0 |                  0 |
|  3 |             93 |      11160 |              7 |              5 |         1968 |             1968 |              0 |           1065 |              0 |          1045 |            2110 |         2110 |            0 |                 0 |          2110 |                1 |                0 |           2 |           1 |               3 |               1 |               8 |            2 |            1968 |             2 |           522 |              0 |               0 |                0 |            0 |              0 |           0 |          0 |         4 |      2010 |                    0 |                    0 |                    1 |                    0 |                    0 |                    0 |                    0 |                  0 |                 1 |                 0 |                 0 |                 0 |                 0 |                  1 |                  0 |                  0 |
|  4 |             74 |      13830 |              5 |              5 |         1997 |             1998 |              0 |            791 |              0 |           137 |             928 |          928 |          701 |                 0 |          1629 |                0 |                0 |           2 |           1 |               3 |               1 |               6 |            1 |            1997 |             2 |           482 |            212 |              34 |                0 |            0 |              0 |           0 |          0 |         3 |      2010 |                    0 |                    0 |                    0 |                    0 |                    0 |                    1 |                    0 |                  0 |                 0 |                 0 |                 0 |                 0 |                 1 |                  1 |                  0 |                  0 |
|  5 |             78 |       9978 |              6 |              6 |         1998 |             1998 |             20 |            602 |              0 |           324 |             926 |          926 |          678 |                 0 |          1604 |                0 |                0 |           2 |           1 |               3 |               1 |               7 |            1 |            1998 |             2 |           470 |            360 |              36 |                0 |            0 |              0 |           0 |          0 |         6 |      2010 |                    0 |                    0 |                    0 |                    0 |                    0 |                    1 |                    0 |                  0 |                 0 |                 0 |                 1 |                 0 |                 0 |                  1 |                  0 |                  0 |
|  6 |             41 |       4920 |              8 |              5 |         2001 |             2001 |              0 |            616 |              0 |           722 |            1338 |         1338 |            0 |                 0 |          1338 |                1 |                0 |           2 |           0 |               2 |               1 |               6 |            0 |            2001 |             2 |           582 |              0 |               0 |              170 |            0 |              0 |           0 |          0 |         4 |      2010 |                    0 |                    0 |                    1 |                    0 |                    0 |                    0 |                    0 |                  0 |                 0 |                 0 |                 1 |                 0 |                 0 |                  1 |                  0 |                  0 |
|  7 |             43 |       5005 |              8 |              5 |         1992 |             1992 |              0 |            263 |              0 |          1017 |            1280 |         1280 |            0 |                 0 |          1280 |                0 |                0 |           2 |           0 |               2 |               1 |               5 |            0 |            1992 |             2 |           506 |              0 |              82 |                0 |            0 |            144 |           0 |          0 |         1 |      2010 |                    0 |                    0 |                    1 |                    0 |                    0 |                    0 |                    0 |                  0 |                 0 |                 0 |                 1 |                 0 |                 0 |                  1 |                  0 |                  0 |
|  8 |             39 |       5389 |              8 |              5 |         1995 |             1996 |              0 |           1180 |              0 |           415 |            1595 |         1616 |            0 |                 0 |          1616 |                1 |                0 |           2 |           0 |               2 |               1 |               5 |            1 |            1995 |             2 |           608 |            237 |             152 |                0 |            0 |              0 |           0 |          0 |         3 |      2010 |                    0 |                    0 |                    1 |                    0 |                    0 |                    0 |                    0 |                  0 |                 0 |                 0 |                 1 |                 0 |                 0 |                  1 |                  0 |                  0 |
|  9 |             60 |       7500 |              7 |              5 |         1999 |             1999 |              0 |              0 |              0 |           994 |             994 |         1028 |          776 |                 0 |          1804 |                0 |                0 |           2 |           1 |               3 |               1 |               7 |            1 |            1999 |             2 |           442 |            140 |              60 |                0 |            0 |              0 |           0 |          0 |         6 |      2010 |                    0 |                    0 |                    0 |                    0 |                    0 |                    1 |                    0 |                  0 |                 0 |                 0 |                 1 |                 0 |                 0 |                  1 |                  0 |                  0 |
 * Link to data [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
 
#### Results
<div class="flourish-embed" data-src="story/214065" data-url="https://public.flourish.studio/story/214065/embed"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

### AG News
 * News articles/headlines (text data)
 * Each news article is categorized as world, sports, business, or sci/tech news
 * Often, text data will be translated into numerical representations so it can be understood by machine learning algorithms. These numbers are known as encodings.
 * To generate encodings, we use a library known as FastText, which translates the text into a vector of 100 numbers. This set of 100 numbers is the high-dimensional data we will reduce to 2 dimensions.
 * Because FastText is not a ground-truth representation of the article, these numbers are less accurate than the other two datasets

Example data:
|    | Text                                                                                                                                                                                                                                                                                                                                                                                                                           |
|---:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|  0 | Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\band of ultra-cynics, are seeing green again.                                                                                                                                                                                                                                                                               |
|  1 | Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\which has a reputation for making well-timed and occasionally\controversial plays in the defense industry, has quietly placed\its bets on another part of the market.                                                                                                                                                     |
|  2 | Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\about the economy and the outlook for earnings are expected to\hang over the stock market next week during the depth of the\summer doldrums.                                                                                                                                                                                       |
|  3 | Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\flows from the main pipeline in southern Iraq after\intelligence showed a rebel militia could strike\infrastructure, an oil official said on Saturday.                                                                                                                                                               |
|  4 | Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.                                                                                                                                                                                     |
|  5 | Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\but stayed near lows for the year as oil prices surged past  #36;46\a barrel, offsetting a positive outlook from computer maker\Dell Inc. (DELL.O)                                                                                                                                                                                |
|  6 | Money Funds Fell in Latest Week (AP) AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday.                                                                                                                                                                                                        |
|  7 | Fed minutes show dissent over inflation (USATODAY.com) USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.                                                                                                                                                                   |
|  8 | Safety Net (Forbes.com) Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, "buying insurance was the furthest thing from my mind," says Riley. |
|  9 | Wall St. Bears Claw Back Into the Black  NEW YORK (Reuters) - Short-sellers, Wall Street's dwindling  band of ultra-cynics, are seeing green again.                                                                                                                                                                                                                                                                            |
 * Link to data [here](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)

#### Results
<div class="flourish-embed" data-src="story/214976" data-url="https://public.flourish.studio/story/214976/embed"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

### MNIST
 * Hand-drawn digits (image data)
 * Images are translated to numerical data with 784 data points, where each data point is a pixel in the image
 * 784 columns for pixels, 1 column for what the hand-drawn digit actually is
 
Example data:
![MNIST image](https://i2.wp.com/syncedreview.com/wp-content/uploads/2019/06/MNIST.png?fit=530%2C297&ssl=1)
 * Link to data [here](https://pjreddie.com/projects/mnist-in-csv/)

#### Results
<div class="flourish-embed" data-src="story/214989" data-url="https://public.flourish.studio/story/214989/embed"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

# Discussion

Each dataset showed significant discrepancies between different methods of dimension reduction. However, this discrepancy was not consistent throughout the different datasets. In the AMES Housing dataset, the more complicated algorithms (T-SNE and UMAP) produced graphs that are visually appealing but hard to interpret, whereas PCA and SVD produced relatively simple graphs. 

For the AG News dataset, no method stood out to be totally different in terms of structure. Although PCA and SVD were very similar, the graphs produced by T-SNE and UMAP were not nearly as complex as those in the AMES Housing dataset. Different parameters for T-SNE and UMAP might have affected how these graphs were produced. For this blog post, we used the default parameters, but tuning parameters can drastically change how these algorithms work. For more information on T-SNE and UMAP parameters, visit [here](https://umap-learn.readthedocs.io/en/latest/parameters.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).

The MNIST dataset most significantly showed the advantages of T-SNE and UMAP over PCA and SVD. Each of the more complicated algorithms produced graphs with clear clusters for each digit. Specifically, UMAP did an excellent job sorting each digit separately.

Generally, PCA and SVD performed similarly, and T-SNE and UMAP performed similarly. Therefore, it’s important to consider largely what makes PCA/SVD different from T-SNE/UMAP. The dataset which shows the largest discrepancy between the simple and complex algorithms is MNIST. This begs the question: what separates MNIST from other datasets we used? Also, why would T-SNE and UMAP perform so much better in this dataset?

One explanation for the discrepancy could be the sparsity and size of the MNIST dataset. There are 784 dimensions in an MNIST image (one number for each pixel), and dimension reduction represents an image with just 2 dimensions. This is much larger than the amount of dimensions for AMES and AG News (81 and 100, respectively). Additionally, the dimensions in an MNIST image are much more sparse than other datasets. Because every white pixel is a 0, almost every data point will be 0. Perhaps PCA and SVD fail to recognize that pattern, and fail with most large and sparse datasets.

Although PCA and SVD performed much worse than T-SNE and UMAP for MNIST, their results in different datasets were good. The datasets for AMES Housing and AG News are relatively consistent datasets, which bolsters the idea that PCA and SVD could perform better with smaller and more consistent datasets. It also seems that T-SNE and UMAP can still perform well in scenarios where PCA and SVD thrive. However, in datasets where PCA/SVD perform similarly to T-SNE/UMAP, the simpler algorithms’ speed and efficiency should give them the edge.



# Concluding Remarks

Dimension reduction is extremely useful for visualization in a wide variety of machine learning applications. Different techniques for dimension reduction can produce very different results, however. 

More complex and modern algorithms seem to produce results that are easier to visualize. However, simple algorithms might be occasionally superior when it comes to simple linear prediction using the reduced dimension. For example, in the AMES Housing dataset, it would be quite easy to use a single line to predict house price based on the PCA and SVD reduced dimensions. Nonetheless, the complex algorithms like UMAP and TSNE seem to reign supreme in most regards otherwise.

Dimension reduction is an easy-to-use and practical solution for visualization of machine learning data. Using it to visualize results is an important part of many efforts in the realm of data science research.
